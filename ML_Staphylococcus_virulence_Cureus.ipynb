{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, Birch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sl4gqPIa0jfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)"
      ],
      "metadata": {
        "id": "eR2p1Fo50m2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "ElQkj2iz2Jq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features (assuming the columns you mentioned are features for clustering)\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]"
      ],
      "metadata": {
        "id": "hbzY4GkhTPhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n"
      ],
      "metadata": {
        "id": "VdtRPo_52cGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the features to improve clustering performance\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n"
      ],
      "metadata": {
        "id": "hb_zs2q-1lve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(scaled_features)\n",
        "\n",
        "# Define unique markers for each feature\n",
        "markers = ['o', 's', '^', 'D', 'v', '*']  # Different symbols for each feature\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y']  # Colors for better visualization\n",
        "\n",
        "# Identify the dominant feature for each sample\n",
        "dominant_feature = np.argmax(features.values, axis=1)  # Index of max feature per row\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i, feature in enumerate(features.columns):\n",
        "    # Select points where the i-th feature is the most dominant\n",
        "    mask = dominant_feature == i\n",
        "    plt.scatter(pca_features[mask, 0], pca_features[mask, 1], marker=markers[i], color=colors[i], label=feature, alpha=0.6)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Visualization with Feature-Based Symbols')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LRDX3UtsSg_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn\n"
      ],
      "metadata": {
        "id": "QEoRVdNNTeji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply UMAP for dimensionality reduction\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_features = umap_model.fit_transform(scaled_features)\n",
        "\n",
        "# Define unique markers and colors for each feature\n",
        "markers = ['o', 's', '^', 'D', 'v', '*']  # Symbols for each feature\n",
        "colors = ['r', 'g', 'b', 'c', 'm', 'y']  # Colors for visualization\n",
        "\n",
        "# Identify the dominant feature for each sample\n",
        "dominant_feature = np.argmax(features.values, axis=1)  # Index of max feature per row\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for i, feature in enumerate(features.columns):\n",
        "    # Select points where the i-th feature is the most dominant\n",
        "    mask = dominant_feature == i\n",
        "    plt.scatter(umap_features[mask, 0], umap_features[mask, 1],\n",
        "                marker=markers[i], color=colors[i],\n",
        "                label=feature, alpha=0.7)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('UMAP Component 1', fontsize=14)\n",
        "plt.ylabel('UMAP Component 2', fontsize=14)\n",
        "plt.title('UMAP Visualization of Features', fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.show()  # No grid for a publication-ready plot\n"
      ],
      "metadata": {
        "id": "frQfg6INTiQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the linkage matrix\n",
        "linkage_matrix = sch.linkage(scaled_features, method='ward')  # 'ward' minimizes variance\n",
        "\n",
        "# Create the figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the dendrogram\n",
        "sch.dendrogram(linkage_matrix, labels=features.index, leaf_rotation=90, leaf_font_size=10, color_threshold=5)\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('Samples', fontsize=14)\n",
        "plt.ylabel('Cluster Distance', fontsize=14)\n",
        "plt.title('Hierarchical Clustering Dendrogram', fontsize=16)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sVavSF00gF3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KMeans Clustering ---\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # You can change the number of clusters\n",
        "kmeans_labels = kmeans.fit_predict(scaled_features)"
      ],
      "metadata": {
        "id": "X1OcbqiL1lnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the KMeans cluster labels to the original data\n",
        "# Create a new column filled with a placeholder (e.g., -1)\n",
        "data['KMeans_Cluster'] = -1\n",
        "\n",
        "# Assign cluster labels to the rows that were used in clustering\n",
        "data.loc[features.index, 'KMeans_Cluster'] = kmeans_labels"
      ],
      "metadata": {
        "id": "DIlA6CQl3E_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the esxA distribution across clusters\n",
        "print(data.groupby('KMeans_Cluster')['EsxA'].value_counts())\n"
      ],
      "metadata": {
        "id": "qdMiytxI4bok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=data['KMeans_Cluster'], y=data['EsxA'], data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('Distribution of EsxA Expression Across Clusters')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rAmWiByD4fMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(features)  # 'features' includes esxA\n",
        "\n",
        "# Get cluster labels for the subset of data used for PCA\n",
        "cluster_labels_for_pca = data.loc[features.index, 'KMeans_Cluster']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Use cluster_labels_for_pca instead of data['KMeans_Cluster']\n",
        "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-Means Clustering with PCA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G1niAPNU5Yrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Plot K-Means clustering with PCA\n",
        "print(\"Explained Variance Ratios:\", pca.explained_variance_ratio_)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k', alpha=0.7, label='Clusters')\n",
        "\n",
        "# Mark each feature direction in PCA space\n",
        "feature_vectors = pca.components_.T  # Get the principal components\n",
        "feature_names = ['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']\n",
        "# Use valid marker styles instead of single letters\n",
        "markers = ['o', 's', '^', 'd', 'v', 'x']  # Example: 'o' for circle, 's' for square, etc.\n",
        "\n",
        "for i, (feature, marker) in enumerate(zip(feature_names, markers)):\n",
        "    plt.scatter(feature_vectors[i, 0] * max(pca_features[:, 0]),\n",
        "                feature_vectors[i, 1] * max(pca_features[:, 1]),\n",
        "                marker=marker, s=100, label=feature, edgecolors='black')\n",
        "\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('K-Means Clustering with PCA & Feature Marking')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1UvNCF4mERHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the plot with high quality\n",
        "plt.savefig('pca_kmeans_clustering.png', dpi=300,)"
      ],
      "metadata": {
        "id": "uHAAU7TaGEhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "!pip install umap-learn  # Install umap-learn\n",
        "# Instead of:\n",
        "# import umap.umap_ as umap\n",
        "import umap # Import umap instead of umap.umap_\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- t-SNE ---\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_features = tsne.fit_transform(scaled_features)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE Clustering Visualization')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "uP1QXEbTEYx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- t-SNE ---\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "tsne_features = tsne.fit_transform(scaled_features)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k', alpha=0.7, label='Clusters')\n",
        "\n",
        "# Mark each feature direction in t-SNE space\n",
        "for i, (feature, marker) in enumerate(zip(feature_names, markers)):\n",
        "    plt.scatter(np.mean(tsne_features[:, 0]) + i * 0.5, np.mean(tsne_features[:, 1]) + i * 0.5,\n",
        "                marker=marker, s=100, label=feature, edgecolors='black')\n",
        "\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE Clustering Visualization with Feature Marking')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pkt7ovx6Fx64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UMAP ---\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_features = umap_reducer.fit_transform(scaled_features)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(umap_features[:, 0], umap_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('UMAP Clustering Visualization')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OBDP6oWqEamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UMAP ---\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "umap_features = umap_reducer.fit_transform(scaled_features)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(umap_features[:, 0], umap_features[:, 1], c=cluster_labels_for_pca, cmap='viridis', edgecolors='k', alpha=0.7, label='Clusters')\n",
        "\n",
        "# Mark each feature direction in UMAP space\n",
        "for i, (feature, marker) in enumerate(zip(feature_names, markers)):\n",
        "    plt.scatter(np.mean(umap_features[:, 0]) + i * 0.5, np.mean(umap_features[:, 1]) + i * 0.5,\n",
        "                marker=marker, s=100, label=feature, edgecolors='black')\n",
        "\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.title('UMAP Clustering Visualization with Feature Marking')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4OadFx83GTeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Line graph to visualize EsxA expression levels across clusters\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(x=data['KMeans_Cluster'], y=data['EsxA'], marker=\"o\", label=\"EsxA Expression\")\n",
        "\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('Trend of EsxA Expression Across Clusters')\n",
        "plt.xticks(ticks=sorted(data['KMeans_Cluster'].unique()))  # Ensure proper x-axis ticks\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j4waI57arwj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot each sample as a separate line\n",
        "for sample in data.index:\n",
        "    plt.plot(['Cluster ' + str(c) for c in [data.loc[sample, 'KMeans_Cluster']]],\n",
        "             [data.loc[sample, 'EsxA']],\n",
        "             marker=\"o\", linestyle=\"-\", alpha=0.7)\n",
        "\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('EsxA Expression Trends for All Samples Across Clusters')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JtmxPWO0sBFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Get unique clusters and sort them\n",
        "unique_clusters = sorted(data['KMeans_Cluster'].unique())\n",
        "\n",
        "# Plot each sample separately\n",
        "for sample in data.index:\n",
        "    cluster = data.loc[sample, 'KMeans_Cluster']\n",
        "    esxA_value = data.loc[sample, 'EsxA']\n",
        "\n",
        "    # Use cluster as x-axis and EsxA as y-axis\n",
        "    plt.plot(cluster, esxA_value, marker=\"o\", linestyle=\"-\", alpha=0.7, label=f\"Sample {sample}\")\n",
        "\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('EsxA Expression Trends for All Samples Across Clusters')\n",
        "plt.xticks(ticks=unique_clusters)  # Set proper x-axis ticks\n",
        "plt.legend(title=\"Samples\", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)  # Add legend outside plot\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3OiV5sWKsbiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Sort data by clusters for correct plotting\n",
        "sorted_data = data.sort_values(by='KMeans_Cluster')\n",
        "\n",
        "# Plot each sample as a separate line across clusters\n",
        "for sample in sorted_data.index:\n",
        "    plt.plot(sorted_data['KMeans_Cluster'], sorted_data['EsxA'], marker=\"o\", linestyle=\"-\", alpha=0.7)\n",
        "\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('EsxA Expression Trends for All Samples Across Clusters')\n",
        "plt.xticks(sorted(data['KMeans_Cluster'].unique()))  # Ensure correct x-axis labels\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KDO3Ckl7svIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "data['KMeans_Cluster'] = data['KMeans_Cluster'].astype(str)  # Convert to string for colors\n",
        "sns.pairplot(data, hue='KMeans_Cluster', diag_kind='kde', palette='viridis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XSp6d24h5xZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(pca_features, annot=True)\n",
        "plt.title(\"Kmeans Cluster Prediction\", fontsize =10)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(pca_features, annot=True)\n",
        "plt.title(\"Hierarchical Clustering Cluster Prediction\", fontsize =10)"
      ],
      "metadata": {
        "id": "VUUjJkFIovhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Convert 'KMeans_Cluster' back to numeric for color mapping\n",
        "plt.scatter(data['Lipase'], data['EsxA'], c=data['KMeans_Cluster'].astype(int), cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('Lipase')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('K-Means Clustering: Lipase vs EsxA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DFNjeCLI6hq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Convert 'KMeans_Cluster' back to numeric for color mapping\n",
        "plt.scatter(data['Protease'], data['EsxA'], c=data['KMeans_Cluster'].astype(int), cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('Protease')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('K-Means Clustering: Protease vs EsxA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0J2zqBUc6ma2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Convert 'KMeans_Cluster' back to numeric for color mapping\n",
        "plt.scatter(data['Hemolysin'], data['EsxA'], c=data['KMeans_Cluster'].astype(int), cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('Hemolysin')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('K-Means Clustering: Hemolysin vs EsxA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0eiw6VRX6vtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Convert 'KMeans_Cluster' back to numeric for color mapping\n",
        "plt.scatter(data['DNase'], data['EsxA'], c=data['KMeans_Cluster'].astype(int), cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('DNase')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('K-Means Clustering: DNase vs EsxA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ssg8lUyX66wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Convert 'KMeans_Cluster' back to numeric for color mapping\n",
        "plt.scatter(data['Staphyloxanthin'], data['EsxA'], c=data['KMeans_Cluster'].astype(int), cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('Staphyloxanthin')\n",
        "plt.ylabel('EsxA Expression')\n",
        "plt.title('K-Means Clustering: Staphyloxanthin vs EsxA')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTsfpslO7WU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hierarchical Clustering ---\n",
        "linked = linkage(scaled_features, method='ward')  # Using 'ward' linkage method\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3-LGG127mgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BIRCH Clustering\n",
        "birch = Birch(n_clusters=3)  # You can change the number of clusters\n",
        "birch_labels = birch.fit_predict(scaled_features)\n",
        "\n",
        "# Create a new column in 'data' and fill with a placeholder (e.g., -1)\n",
        "data['BIRCH_Cluster'] = -1\n",
        "\n",
        "# Assign cluster labels to the rows that were used in clustering\n",
        "data.loc[features.index, 'BIRCH_Cluster'] = birch_labels # Assign to the same rows used for features\n",
        "\n",
        "# --- View the final data with clusters ---\n",
        "print(data.head())\n",
        "\n",
        "# Optionally, save the data with clusters to a new CSV\n",
        "data.to_csv('clustered_data.csv', index=False)"
      ],
      "metadata": {
        "id": "GM3maVLL8GBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer  # Import the imputer\n",
        "\n",
        "# Selecting all features for clustering\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Impute missing values (e.g., using the mean)\n",
        "imputer = SimpleImputer(strategy='mean')  # Create an imputer instance\n",
        "features_imputed = imputer.fit_transform(features)  # Impute missing values\n",
        "\n",
        "# Reduce dimensionality to 2D\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(features_imputed)  # Use imputed features\n",
        "\n",
        "# Scatter plot of PCA components\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(pca_features[:, 0], pca_features[:, 1], c=data['BIRCH_Cluster'], cmap='viridis', edgecolors='k')\n",
        "\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.title('PCA Visualization of BIRCH Clusters')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNorJ89q85H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "data['BIRCH_Cluster'] = data['BIRCH_Cluster'].astype(str)  # Convert to string for color coding\n",
        "\n",
        "sns.pairplot(data, hue='BIRCH_Cluster', diag_kind='kde', palette='viridis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fAkOl8529Gxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###############"
      ],
      "metadata": {
        "id": "djtUwraX-Inx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas seaborn matplotlib\n"
      ],
      "metadata": {
        "id": "X3geD58TAjNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Separate Staphyloxanthin from other features\n",
        "numerical_data = data.drop(columns=['Staphyloxanthin'])\n",
        "staphyloxanthin_scores = data[['Staphyloxanthin']]\n",
        "\n",
        "# Create heatmap for numerical data\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numerical_data, cmap='YlGnBu', linewidths=0.5, fmt='.2f', cbar=True)\n",
        "plt.title('Heatmap of Virulence Factors')\n",
        "plt.show()\n",
        "\n",
        "# Create a separate heatmap for Staphyloxanthin scores\n",
        "plt.figure(figsize=(10, 1))  # A smaller figure for categorical data\n",
        "sns.heatmap(staphyloxanthin_scores.T, cmap='viridis', cbar=False, fmt='d')\n",
        "plt.title('Staphyloxanthin Scores')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Eo-U44CWFO4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from sklearn.impute import SimpleImputer  # Import SimpleImputer\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Separate Staphyloxanthin from other features\n",
        "numerical_data = data.drop(columns=['Staphyloxanthin'])  # Remove categorical/non-numerical column\n",
        "staphyloxanthin_scores = data[['Staphyloxanthin']]  # Store separately if needed\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # Replace NaN with the mean of the column\n",
        "numerical_data_imputed = imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame\n",
        "numerical_data_imputed = pd.DataFrame(numerical_data_imputed, columns=numerical_data.columns)\n",
        "\n",
        "\n",
        "# Perform hierarchical clustering using 'ward' method\n",
        "row_linkage = linkage(numerical_data_imputed, method='ward')  # Cluster samples using imputed data\n",
        "col_linkage = linkage(numerical_data_imputed.T, method='ward')  # Cluster features using imputed data\n",
        "\n",
        "# Create a clustered heatmap\n",
        "g = sns.clustermap(numerical_data_imputed, cmap='YlGnBu', linewidths=0.5,\n",
        "                   row_linkage=row_linkage, col_linkage=col_linkage,\n",
        "                   figsize=(10, 8), cbar=True, fmt='.2f')\n",
        "\n",
        "plt.title('Hierarchical Clustering Heatmap', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xz2hSa2wDrmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Separate Staphyloxanthin from other features\n",
        "numerical_data = data.drop(columns=['Staphyloxanthin'])  # Remove categorical/non-numerical column\n",
        "staphyloxanthin_scores = data[['Staphyloxanthin']]  # Store separately if needed\n",
        "\n",
        "# Impute missing values using SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # Replace NaN with the mean of the column\n",
        "numerical_data_imputed = imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame\n",
        "numerical_data_imputed = pd.DataFrame(numerical_data_imputed, columns=numerical_data.columns, index=numerical_data.index)\n",
        "\n",
        "# Perform hierarchical clustering using 'ward' method\n",
        "row_linkage = linkage(numerical_data_imputed, method='ward')\n",
        "col_linkage = linkage(numerical_data_imputed.T, method='ward')\n",
        "\n",
        "# Create a clustered heatmap with all labels visible\n",
        "g = sns.clustermap(numerical_data_imputed, cmap='YlGnBu', linewidths=0.5,\n",
        "                   row_linkage=row_linkage, col_linkage=col_linkage,\n",
        "                   figsize=(12, 10), cbar=True, annot=False, fmt='.2f',\n",
        "                   dendrogram_ratio=(0.1, 0.2),  # Adjust dendrogram size for better visibility\n",
        "                   xticklabels=True, yticklabels=True)  # Ensure all labels are displayed\n",
        "\n",
        "plt.title('Hierarchical Clustering Heatmap', fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5vU5JOiYE_0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv', header=0)  # Replace with your actual CSV file\n",
        "\n",
        "# Assuming you have already performed clustering and assigned labels\n",
        "# If you used K-Means clustering:\n",
        "\n",
        "# Choose the number of clusters based on your previous results\n",
        "n_clusters = 3  # Adjust this based on your best clustering result\n",
        "\n",
        "# Create an imputer to fill NaN values with the mean of each column\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform the imputer on your data (excluding the first column if it's a header)\n",
        "data_imputed = imputer.fit_transform(df.iloc[:, 1:])\n",
        "\n",
        "# Initialize KMeans with the imputed data\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "df[\"Cluster\"] = kmeans.fit_predict(data_imputed)  # Use imputed data for clustering\n",
        "\n",
        "# Melt data for violin plot\n",
        "df_melted = df.melt(id_vars=[\"Cluster\"], var_name=\"Feature\", value_name=\"Value\")\n",
        "\n",
        "# Plot violin plots for all features\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x=\"Feature\", y=\"Value\", hue=\"Cluster\", data=df_melted, palette=\"Set2\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Violin Plot of Features Across Clusters\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p-VH5Mai33HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########################"
      ],
      "metadata": {
        "id": "NNijZ1Bd6kg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "y = data['EsxA']\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature Importance Plot\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=importances, y=feature_names, palette='viridis')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance using Random Forest\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kxI9n7SyBkKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format (Assuming values indicate connections)\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "chord_data = hv.Chord(data)\n",
        "\n",
        "# Styling the Chord Diagram\n",
        "chord_data.opts(opts.Chord(cmap='Category10', edge_color='source', labels='name', node_color='index'))\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)\n"
      ],
      "metadata": {
        "id": "f6mb9-ouqNoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "from bokeh.models import ColorBar, LinearColorMapper\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format for Chord Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "chord_data = hv.Chord(data)\n",
        "\n",
        "# Define color mapping\n",
        "color_mapper = LinearColorMapper(palette='Viridis256', low=min([d[2] for d in data]), high=max([d[2] for d in data]))\n",
        "\n",
        "# Styling the Chord Diagram\n",
        "chord_data.opts(\n",
        "    opts.Chord(\n",
        "        cmap='Category10',      # Node colors\n",
        "        edge_color='value',     # Color edges based on interaction strength\n",
        "        labels='name',          # Add labels to nodes\n",
        "        node_color='index',     # Color nodes differently\n",
        "        line_width=2,           # Increase line thickness\n",
        "        edge_alpha=0.8,         # Edge transparency\n",
        "        width=800, height=800   # Adjust figure size\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)\n"
      ],
      "metadata": {
        "id": "zy83RHF0rkPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "from bokeh.models import ColorBar, LinearColorMapper\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format for Chord Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "chord_data = hv.Chord(data)\n",
        "\n",
        "# Define color mapping\n",
        "color_mapper = LinearColorMapper(palette='Viridis256', low=min([d[2] for d in data]), high=max([d[2] for d in data]))\n",
        "\n",
        "# Styling the Chord Diagram\n",
        "chord_data.opts(\n",
        "    opts.Chord(\n",
        "        cmap='Category10',      # Node colors\n",
        "        edge_color='value',     # Color edges based on interaction strength\n",
        "        labels='name',          # Add labels to nodes\n",
        "        node_color='index',     # Color nodes differently\n",
        "        edge_line_width=2,      # Changed line_width to edge_line_width to control edge thickness\n",
        "        edge_alpha=0.8,         # Edge transparency\n",
        "        width=800, height=800   # Adjust figure size\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)"
      ],
      "metadata": {
        "id": "uBMTUaEwr6A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas holoviews bokeh numpy\n"
      ],
      "metadata": {
        "id": "TgR6VXrhspgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "import numpy as np\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format for Chord Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "nodes = list(set([d[0] for d in data] + [d[1] for d in data]))\n",
        "node_mapping = {name: i for i, name in enumerate(nodes)}\n",
        "\n",
        "# Assign random colors to edges for better visualization\n",
        "colors = hv.Cycle('Category20')  # Choose from 20 different colors\n",
        "\n",
        "# Convert to Holoviews Chord format\n",
        "chord_data = hv.Chord((data, hv.Dataset(nodes, 'index')))\n",
        "\n",
        "# Apply Styling\n",
        "chord_data.opts(\n",
        "    opts.Chord(\n",
        "        cmap='Category20',      # Color scheme for nodes\n",
        "        edge_color=hv.dim('value'),  # Color edges based on interaction strength\n",
        "        edge_cmap='Viridis',    # Gradient color for edges\n",
        "        labels='index',         # Show node labels\n",
        "        node_color='index',     # Different color for each node\n",
        "        node_size=15,           # Make nodes larger\n",
        "        edge_line_width=2,      # Thicker edges\n",
        "        edge_alpha=0.8,         # Transparency for better visibility\n",
        "        width=800, height=800   # Adjust figure size\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)\n"
      ],
      "metadata": {
        "id": "6Po82g8lssS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format for Chord Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "nodes = list(set([d[0] for d in data] + [d[1] for d in data]))\n",
        "node_mapping = {name: i for i, name in enumerate(nodes)}\n",
        "\n",
        "# Convert to Holoviews Chord format\n",
        "chord_data = hv.Chord((data, hv.Dataset(nodes, 'index')))\n",
        "\n",
        "# Apply Styling\n",
        "chord_data.opts(\n",
        "    opts.Chord(\n",
        "        cmap='Category20',           # Different colors for nodes\n",
        "        edge_cmap='Plasma',          # Gradient color for edges (instead of black)\n",
        "        edge_color=hv.dim('value'),  # Map color to interaction strength\n",
        "        labels='index',              # Show node labels\n",
        "        node_color='index',          # Different color for each node\n",
        "        node_size=15,                # Increase node size\n",
        "        edge_line_width=2,           # Make edges thicker\n",
        "        edge_alpha=0.8,              # Transparency for better visualization\n",
        "        width=800, height=800        # Adjust figure size\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)\n"
      ],
      "metadata": {
        "id": "2oFgUODLtmxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv',header=0)\n",
        "\n",
        "# Convert data to long format for Chord Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = df.iloc[:, i].sum() + df.iloc[:, j].sum()  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Convert to Chord format\n",
        "nodes = list(set([d[0] for d in data] + [d[1] for d in data]))\n",
        "node_mapping = {name: i for i, name in enumerate(nodes)}\n",
        "\n",
        "# Convert to Holoviews Chord format\n",
        "chord_data = hv.Chord((data, hv.Dataset(nodes, 'index')))\n",
        "\n",
        "# Apply Styling\n",
        "chord_data.opts(\n",
        "    opts.Chord(\n",
        "        cmap='Category20',           # Different colors for nodes\n",
        "        edge_cmap='Plasma',          # ðŸŒˆ Gradient color for edges\n",
        "        edge_color=hv.dim('value'),  # Map color to interaction strength\n",
        "        labels='index',              # Show node labels\n",
        "        node_color='index',          # Different color for each node\n",
        "        node_size=15,                # Increase node size\n",
        "        edge_line_width=3,           # Make edges thicker\n",
        "        edge_alpha=0.9,              # Slight transparency\n",
        "        width=800, height=800        # Adjust figure size\n",
        "    )\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(chord_data)\n"
      ],
      "metadata": {
        "id": "sqOacH_hwKh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "import numpy as np\n",
        "hv.extension('bokeh')\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Convert data to long format for Sankey Diagram\n",
        "data = []\n",
        "columns = df.columns\n",
        "\n",
        "for i in range(len(columns)):\n",
        "    for j in range(i + 1, len(columns)):\n",
        "        value = np.random.randint(1, 100)  # Example interaction strength\n",
        "        if value > 0:  # If there's interaction\n",
        "            data.append((columns[i], columns[j], value))\n",
        "\n",
        "# Create node list\n",
        "nodes = list(set([d[0] for d in data] + [d[1] for d in data]))\n",
        "node_mapping = {name: i for i, name in enumerate(nodes)}\n",
        "# Convert to Holoviews Sankey format\n",
        "edges = [(node_mapping[d[0]], node_mapping[d[1]], d[2]) for d in data]\n",
        "# The change is here, we explicitly declare 'name' as a kdim\n",
        "nodes = hv.Dataset([(i, name) for name, i in node_mapping.items()], 'index', 'name')\n",
        "\n",
        "# Creating Sankey diagram (alternative to Chord)\n",
        "sankey = hv.Sankey((edges, nodes)).opts(\n",
        "    cmap='Category20',             # Unique color for each node\n",
        "    edge_cmap='Plasma',            # Colored inside curves\n",
        "    edge_color=hv.dim('value'),    # Map edge color to interaction strength\n",
        "    labels='name',                 # Show node labels\n",
        "    node_color='index',            # Assign unique colors to nodes\n",
        "    node_size=20,                  # Increase node size for visibility\n",
        "    edge_line_width=3,             # Thicker edges\n",
        "    edge_alpha=0.8,                # Slight transparency\n",
        "    width=900, height=900          # Adjust figure size\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "hv.output(sankey)\n"
      ],
      "metadata": {
        "id": "Ixz1MdmTy5Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_chord_diagram import chord_diagram\n",
        "\n",
        "# Load CSV data\n",
        "df = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Generate a random matrix for interactions\n",
        "size = len(df.columns)\n",
        "matrix = np.random.randint(1, 100, size=(size, size))\n",
        "\n",
        "# Define labels\n",
        "labels = df.columns.tolist()\n",
        "\n",
        "# Define colors for inside curved edges\n",
        "cmap = plt.get_cmap(\"rainbow\")  # ðŸŒˆ Different colors for curves\n",
        "colors = [cmap(i / size) for i in range(size)]\n",
        "\n",
        "# Create Chord Diagram\n",
        "fig, ax = plt.subplots(figsize=(10, 10), dpi=300)\n",
        "chord_diagram(matrix, names=labels, ax=ax, cmap=\"rainbow\", directed=False, chord_colors=colors)\n",
        "\n",
        "# Adjust and show plot\n",
        "plt.title(\"Circular Chord Diagram with Colored Inside Curves\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5uVCpuyizyPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpl-chord-diagram # install mpl_chord_diagram module"
      ],
      "metadata": {
        "id": "P8RK2vqpz_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_chord_diagram import chord_diagram # import chord_diagram\n",
        "\n",
        "# Load CSV data\n",
        "df = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Generate a random matrix for interactions\n",
        "size = len(df.columns)\n",
        "matrix = np.random.randint(1, 100, size=(size, size))\n",
        "\n",
        "# Define labels\n",
        "labels = df.columns.tolist()\n",
        "\n",
        "# Define colors for inside curved edges\n",
        "cmap = plt.get_cmap(\"rainbow\")  # ðŸŒˆ Different colors for curves\n",
        "colors = [cmap(i / size) for i in range(size)]\n",
        "\n",
        "# Create Chord Diagram\n",
        "fig, ax = plt.subplots(figsize=(10, 10), dpi=300)\n",
        "chord_diagram(matrix, names=labels, ax=ax, cmap=\"rainbow\", directed=False, chord_colors=colors)\n",
        "\n",
        "# Adjust and show plot\n",
        "plt.title(\"Circular Chord Diagram with Colored Inside Curves\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AMHkglOe0DHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_chord_diagram import chord_diagram\n",
        "\n",
        "# Load CSV data\n",
        "df = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Generate a random matrix for interactions (you can replace this with your actual data)\n",
        "size = len(df.columns)\n",
        "matrix = np.random.randint(1, 100, size=(size, size))\n",
        "\n",
        "# Define labels (same as your dataframe columns)\n",
        "labels = df.columns.tolist()\n",
        "\n",
        "# Define colors for inside curved edges using a colormap\n",
        "cmap = plt.get_cmap(\"rainbow\")  # ðŸŒˆ Different colors for curves\n",
        "colors = [cmap(i / size) for i in range(size)]\n",
        "\n",
        "# Create Chord Diagram\n",
        "fig, ax = plt.subplots(figsize=(10, 10), dpi=300)\n",
        "chord_diagram(matrix, names=labels, ax=ax, cmap=\"rainbow\", directed=False, chord_colors=colors)\n",
        "\n",
        "# Add a color bar to the plot\n",
        "sm = plt.cm.ScalarMappable(cmap=\"rainbow\", norm=plt.Normalize(vmin=matrix.min(), vmax=matrix.max()))\n",
        "sm.set_array([])  # Empty array is fine for color bar\n",
        "cbar = fig.colorbar(sm, ax=ax, orientation='vertical', pad=0.01)\n",
        "cbar.set_label('Interaction Strength')\n",
        "\n",
        "# Adjust plot layout and display\n",
        "plt.title(\"Circular Chord Diagram with Colored Inside Curves\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R0nyjhbW8sdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features for clustering\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert back to DataFrame for plotting\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "# Plot a line graph for all samples\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(scaled_df.shape[0]):  # Iterate over rows (samples)\n",
        "    plt.plot(scaled_df.columns, scaled_df.iloc[i, :], marker='o', linestyle='-')\n",
        "\n",
        "plt.xlabel('Virulence Factors')\n",
        "plt.ylabel('Standardized Value')\n",
        "plt.title('Line Graph of All Samples')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uc2wdwXKtbDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features for clustering\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert back to DataFrame for plotting\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "# Generate a color palette\n",
        "num_samples = scaled_df.shape[0]\n",
        "colors = sns.color_palette(\"husl\", num_samples)  # Using seaborn color palette\n",
        "\n",
        "# Plot a line graph for all samples with different colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    plt.plot(scaled_df.columns, scaled_df.iloc[i, :], marker='o', linestyle='-', color=colors[i], label=f'Sample {i+1}')\n",
        "\n",
        "plt.xlabel('Virulence Factors')\n",
        "plt.ylabel('Standardized Value')\n",
        "plt.title('Line Graph of All Samples with Color Chart')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Add legend outside the plot\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"Samples\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CgWedt2Etq_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features for clustering\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert back to DataFrame for plotting\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "\n",
        "# Generate a color palette\n",
        "num_samples = scaled_df.shape[0]\n",
        "colors = sns.color_palette(\"husl\", num_samples)  # Unique colors for each sample\n",
        "\n",
        "# Plot a line graph for all samples with different colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i in range(num_samples):\n",
        "    plt.plot(scaled_df.columns, scaled_df.iloc[i, :], marker='o', linestyle='-', color=colors[i], label=f'Sample {i+1}')\n",
        "\n",
        "plt.xlabel('Virulence Factors')\n",
        "plt.ylabel('Standardized Value')\n",
        "plt.title('Line Graph of All Samples with Color Chart')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# **Modify Legend Position and Formatting**\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), title=\"Samples\", ncol=2)  # Adjust ncol for better readability\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DOuy6-3Yu4td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features and target (Assume EsxA is the target, change as needed)\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change this if needed\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf.feature_importances_\n",
        "feature_names = features.columns\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# **Plot feature importance as a bar chart**\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df, palette=\"viridis\")\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# **Optional: Line Graph for Feature Importance**\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(importance_df['Feature'], importance_df['Importance'], marker='o', linestyle='-', color='b')\n",
        "plt.title('Feature Importance (Line Graph)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mRAgNKpYucn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features and target (Use EsxA as a target or change as needed)\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change this if needed\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Model with more estimators to create a \"forest\" effect\n",
        "rf = RandomForestRegressor(n_estimators=500, random_state=42, max_depth=10)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores for each tree in the forest\n",
        "n_trees = len(rf.estimators_)\n",
        "feature_importance_matrix = np.zeros((n_trees, len(features.columns)))\n",
        "\n",
        "for i, tree in enumerate(rf.estimators_):\n",
        "    feature_importance_matrix[i, :] = tree.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "importance_df = pd.DataFrame(feature_importance_matrix, columns=features.columns)\n",
        "\n",
        "# **Plot Feature Importance as a \"Forest-like\" Graph**\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(data=importance_df, palette=\"Set2\")  # Creates a violin plot (tree-like structure)\n",
        "plt.title('Random Forest Feature Importance (Forest-like View)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3A3TG-9Fv8r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Select a single tree from the forest\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(rf.estimators_[0], feature_names=features.columns, filled=True, rounded=True)\n",
        "plt.title(\"Single Decision Tree from Random Forest\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7aiMMdfFwXns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features and target\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change as needed\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Model\n",
        "rf = RandomForestRegressor(n_estimators=1000, random_state=42, max_depth=5)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances and confidence intervals\n",
        "importances = rf.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
        "\n",
        "# Sort features by importance\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "# **Plot the \"Forest-Like\" Random Forest Feature Importance**\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(range(len(indices)), importances[indices], xerr=std[indices], color='forestgreen', alpha=0.7)\n",
        "plt.yticks(range(len(indices)), [features.columns[i] for i in indices])\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Random Forest Feature Importance (Meta-Analysis Style)\")\n",
        "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EX3BCZ6nwyXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=importances, y=features.columns, palette=\"Greens_r\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Random Forest Feature Importance (Tree Trunk View)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YCkCzH2Xw8ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features and target\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Adjust target column based on your dataset\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions and standard deviation for each sample\n",
        "predictions = rf.predict(X_test)\n",
        "std_dev = np.std([tree.predict(X_test) for tree in rf.estimators_], axis=0)\n",
        "\n",
        "# Create a forest plot-style visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions, y=range(len(predictions)), color='green', label=\"Prediction\")  # Predictions\n",
        "plt.errorbar(predictions, range(len(predictions)), xerr=std_dev, fmt='o', color='black', label=\"95% CI\")  # Error bars\n",
        "plt.axvline(x=np.mean(predictions), color='red', linestyle='--', label=\"Mean Prediction\")\n",
        "\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Random Forest Predictions for Individual Samples (Forest Plot Style)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VpCmlg0ixf_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Get feature importances for all trees\n",
        "feature_importances = np.array([tree.feature_importances_ for tree in rf.estimators_])\n",
        "\n",
        "# Convert to DataFrame\n",
        "importance_df = pd.DataFrame(feature_importances, columns=features.columns)\n",
        "\n",
        "# Plot using a violin plot (mimics trees in a forest)\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(data=importance_df, palette=\"Greens\")  # Green color to resemble trees\n",
        "plt.title('Random Forest Feature Importance (Forest-Like View)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CITmUl39xrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features and target\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change this based on your dataset\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions and standard deviation for each sample\n",
        "predictions = rf.predict(X_test)\n",
        "std_dev = np.std([tree.predict(X_test) for tree in rf.estimators_], axis=0)\n",
        "\n",
        "# Generate unique colors for each sample\n",
        "colors = sns.color_palette(\"husl\", len(predictions))  # Husl gives distinct colors\n",
        "\n",
        "# Create a forest plot-style visualization\n",
        "plt.figure(figsize=(12, 7))\n",
        "for i in range(len(predictions)):\n",
        "    plt.errorbar(predictions[i], i, xerr=std_dev[i], fmt='o', color=colors[i], label=f\"Sample {i+1}\" if i < 10 else \"\")  # Show legend only for first 10\n",
        "\n",
        "# Add mean prediction line\n",
        "plt.axvline(x=np.mean(predictions), color='red', linestyle='--', label=\"Mean Prediction\")\n",
        "\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Random Forest Predictions for All Samples (Forest Plot Style)\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize=\"small\", ncol=2)  # Legend outside to avoid clutter\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vv8bIxKjx6bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features and target\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change based on your dataset\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Train Random Forest on all samples (No train-test split)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(features_scaled, target)\n",
        "\n",
        "# Get predictions and standard deviation for all samples\n",
        "predictions = rf.predict(features_scaled)\n",
        "std_dev = np.std([tree.predict(features_scaled) for tree in rf.estimators_], axis=0)\n",
        "\n",
        "# Generate unique colors for each sample\n",
        "colors = sns.color_palette(\"husl\", len(predictions))  # Distinct colors for each sample\n",
        "\n",
        "# Create a forest plot-style visualization\n",
        "plt.figure(figsize=(12, 7))\n",
        "for i in range(len(predictions)):\n",
        "    plt.errorbar(predictions[i], i, xerr=std_dev[i], fmt='o', color=colors[i], label=f\"Sample {i+1}\" if i < 15 else \"\")  # Show legend for first 15\n",
        "\n",
        "# Add mean prediction line\n",
        "plt.axvline(x=np.mean(predictions), color='red', linestyle='--', label=\"Mean Prediction\")\n",
        "\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Random Forest Predictions for All Samples (Forest Plot Style)\")\n",
        "\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-nso62qWyKnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features and target\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin']]\n",
        "target = data['EsxA']  # Change based on your dataset\n",
        "\n",
        "# Remove missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Train Random Forest on all samples (No train-test split)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(features_scaled, target)\n",
        "\n",
        "# Get predictions and standard deviation for all samples\n",
        "predictions = rf.predict(features_scaled)\n",
        "std_dev = np.std([tree.predict(features_scaled) for tree in rf.estimators_], axis=0)\n",
        "\n",
        "# Generate unique colors for each sample\n",
        "num_samples = len(predictions)\n",
        "colors = sns.color_palette(\"husl\", num_samples)  # Distinct color for each sample\n",
        "\n",
        "# Create a forest plot-style visualization\n",
        "plt.figure(figsize=(12, 7))\n",
        "for i in range(num_samples):\n",
        "    plt.errorbar(predictions[i], i, xerr=std_dev[i], fmt='o', color=colors[i], label=f\"Sample {i+1}\")\n",
        "\n",
        "# Add mean prediction line\n",
        "plt.axvline(x=np.mean(predictions), color='red', linestyle='--', label=\"Mean Prediction\")\n",
        "\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.title(\"Random Forest Predictions for All Samples (Forest Plot Style)\")\n",
        "\n",
        "# Place legend outside and break into multiple columns\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), fontsize=\"small\", ncol=5)  # Adjust columns for better readability\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "L5UoC1xcytIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(rf, features_scaled)\n",
        "shap_values = explainer(features_scaled)\n",
        "\n",
        "shap.summary_plot(shap_values, features, plot_type=\"dot\")\n"
      ],
      "metadata": {
        "id": "TEUaQAmOzfWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(predictions, kde=True, bins=20, color=\"skyblue\")\n",
        "plt.axvline(np.mean(predictions), color='red', linestyle='--', label=\"Mean\")\n",
        "plt.xlabel(\"Predicted Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_sfATs4Kzyha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(rf, features_scaled)\n",
        "shap_values = explainer(features_scaled)\n",
        "\n",
        "# Pick one sample (e.g., sample index 10)\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values[10].values, features.iloc[10])\n"
      ],
      "metadata": {
        "id": "kSrLWd2I0Dw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install umap-learn # install the umap-learn package\n",
        "import umap\n",
        "import numpy as np\n",
        "\n",
        "reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')\n",
        "\n",
        "# Before applying UMAP, replace NaN values with a suitable value\n",
        "# For example, you can replace them with the mean of each column\n",
        "features_scaled = np.nan_to_num(features_scaled, nan=np.nanmean(features_scaled, axis=0))\n",
        "\n",
        "embedding = reducer.fit_transform(features_scaled)\n",
        "\n",
        "# Rest of the plotting code\n",
        "\n",
        "reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean')\n",
        "embedding = reducer.fit_transform(features_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=predictions, cmap='coolwarm', s=50, alpha=0.8)\n",
        "plt.colorbar(label=\"Prediction Value\")\n",
        "plt.title(\"UMAP Projection of Feature Space\")\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9dBZFwS0XWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from joypy import joyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor  # Assuming this is the model you used\n",
        "\n",
        "# ... (your existing code to train the RandomForestRegressor rf) ...\n",
        "\n",
        "# Calculate predictions using the fitted model (rf) on your data\n",
        "predictions = rf.predict(features_scaled)  # Replace 'features_scaled' with your actual data\n",
        "\n",
        "# **Create df_plot DataFrame with your features**\n",
        "# Assuming 'features' is your DataFrame with the features used for prediction:\n",
        "df_plot = features.copy() # Create a copy of the features dataframe and assign it to df_plot\n",
        "\n",
        "# Ensure predictions and df_plot have compatible shapes\n",
        "predictions = predictions[:len(df_plot)]  # Trim predictions to match df_plot length\n",
        "\n",
        "df_plot[\"Predictions\"] = predictions  # Add predictions to dataframe\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=\"Predictions\",\n",
        "    colormap=plt.cm.coolwarm,  # Use plt.cm.coolwarm instead of \"coolwarm\"\n",
        "    figsize=(10, 6)\n",
        ")\n",
        "plt.title(\"Ridge Plot: Prediction Distribution Across Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OAZGsFz017Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot[\"Predictions_Binned\"] = pd.qcut(predictions, q=10, labels=False)  # 10 bins\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=\"Predictions_Binned\",\n",
        "    colormap=plt.cm.coolwarm,\n",
        "    figsize=(10, 6)\n",
        ")\n",
        "plt.title(\"Ridge Plot with Binned Predictions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "udcWKNVT2WPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from joypy import joyplot\n",
        "\n",
        "# Bin the predictions into 10 categories\n",
        "df_plot[\"Predictions_Binned\"] = pd.qcut(predictions, q=10, labels=False)\n",
        "\n",
        "# Define colormap\n",
        "colormap = plt.cm.coolwarm\n",
        "num_bins = df_plot[\"Predictions_Binned\"].nunique()\n",
        "colors = colormap(np.linspace(0, 1, num_bins))\n",
        "\n",
        "# Create legend patches\n",
        "patches = [mpatches.Patch(color=colors[i], label=f\"Bin {i}\") for i in range(num_bins)]\n",
        "\n",
        "# Plot ridge plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=\"Predictions_Binned\",\n",
        "    colormap=colormap,\n",
        "    figsize=(10, 6)\n",
        ")\n",
        "\n",
        "# Add legend\n",
        "ax.legend(handles=patches, title=\"Prediction Bins\", loc=\"upper right\", fontsize=8)\n",
        "\n",
        "plt.title(\"Ridge Plot with Binned Predictions and Color Legend\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FPUaG7Ox3ViK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from joypy import joyplot\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features\n",
        "features = ['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']\n",
        "df_plot = data[features].dropna()  # Drop missing values\n",
        "\n",
        "# Define colormap\n",
        "colormap = plt.cm.coolwarm\n",
        "num_features = len(features)\n",
        "colors = colormap(np.linspace(0, 1, num_features))\n",
        "\n",
        "# Create legend patches\n",
        "patches = [mpatches.Patch(color=colors[i], label=features[i]) for i in range(num_features)]\n",
        "\n",
        "# Create ridge plot\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=None,  # No grouping, features are used as categories\n",
        "    colormap=colormap,\n",
        "    figsize=(12, 6),\n",
        "    overlap=1.2\n",
        ")\n",
        "\n",
        "# Add color legend\n",
        "ax.legend(handles=patches, title=\"Features\", loc=\"upper right\", fontsize=8)\n",
        "\n",
        "plt.title(\"Ridge Plot of Feature Distributions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "72Z6YrE63hPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joypy # Install the joypy library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from joypy import joyplot\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features\n",
        "features = ['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']\n",
        "df_plot = data[features].dropna()  # Drop missing values\n",
        "\n",
        "# Define colormap\n",
        "colormap = plt.cm.coolwarm\n",
        "num_features = len(features)\n",
        "colors = colormap(np.linspace(0, 1, num_features))\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "# Create ridge plot\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=None,  # No grouping, features are used as categories\n",
        "    colormap=colormap,\n",
        "    figsize=(6, 3),\n",
        "    overlap=1.2,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Add color legend inside the same plot\n",
        "patches = [mpatches.Patch(color=colors[i], label=features[i]) for i in range(num_features)]\n",
        "plt.legend(handles=patches, title=\"Features\", loc=\"upper right\", fontsize=6, bbox_to_anchor=(1.2, 1))\n",
        "\n",
        "# Set title\n",
        "plt.title(\"Ridge Plot of Feature Distributions with Color Legend\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bl4ELfbA4R9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joypy  # Install the joypy library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from joypy import joyplot\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features\n",
        "features = ['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']\n",
        "df_plot = data[features].dropna()  # Drop missing values\n",
        "\n",
        "# Define colormap\n",
        "colormap = plt.cm.coolwarm\n",
        "num_features = len(features)\n",
        "colors = colormap(np.linspace(0, 1, num_features))\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "# Create ridge plot\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=None,  # No grouping, features are used as categories\n",
        "    colormap=colormap,\n",
        "    figsize=(6, 3),\n",
        "    overlap=1.2,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Add color legend outside the plot\n",
        "patches = [mpatches.Patch(color=colors[i], label=features[i]) for i in range(num_features)]\n",
        "plt.legend(handles=patches, title=\"Features\", loc=\"upper left\", fontsize=8, bbox_to_anchor=(1.05, 1))\n",
        "\n",
        "# Adjust layout to prevent cutting off the legend\n",
        "plt.subplots_adjust(right=0.75)\n",
        "\n",
        "# Set title\n",
        "plt.title(\"Ridge Plot of Feature Distributions with Color Legend\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lzW5QZWlJasS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joypy  # Install the joypy library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from joypy import joyplot\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Select features\n",
        "features = ['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']\n",
        "df_plot = data[features].dropna()  # Drop missing values\n",
        "\n",
        "# Define colormap\n",
        "colormap = plt.cm.coolwarm\n",
        "num_features = len(features)\n",
        "colors = colormap(np.linspace(0, 1, num_features))\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "# Create ridge plot\n",
        "joyplot(\n",
        "    data=df_plot,\n",
        "    by=None,  # No grouping, features are used as categories\n",
        "    colormap=colormap,\n",
        "    figsize=(6, 3),\n",
        "    overlap=1.2,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Add a smaller legend outside the plot\n",
        "patches = [mpatches.Patch(color=colors[i], label=features[i]) for i in range(num_features)]\n",
        "plt.legend(\n",
        "    handles=patches, title=\"Features\", loc=\"upper left\",\n",
        "    fontsize=6, title_fontsize=7, bbox_to_anchor=(1.05, 1)\n",
        ")\n",
        "\n",
        "# Adjust layout to prevent legend cutoff\n",
        "plt.subplots_adjust(right=0.75)\n",
        "\n",
        "# Set title\n",
        "plt.title(\"Ridge Plot of Feature Distributions with Color Legend\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A4w_PQn2JtxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#########Elbow##########"
      ],
      "metadata": {
        "id": "Bxy343hZ6V3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from kneed import KneeLocator\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Perform PCA (replace with actual scaled features)\n",
        "pca_result = PCA().fit(scaled_features)  # Ensure 'scaled_features' is defined\n",
        "\n",
        "# Compute variance explained for each PC\n",
        "scree_data = pd.DataFrame({\n",
        "    'PC': range(1, len(pca_result.explained_variance_) + 1),\n",
        "    'Variance': pca_result.explained_variance_ratio_\n",
        "})\n",
        "\n",
        "# Find the elbow point\n",
        "knee_locator = KneeLocator(scree_data['PC'], scree_data['Variance'], curve=\"concave\", direction=\"decreasing\")\n",
        "elbow_pc = knee_locator.elbow  # Optimal number of PCs\n",
        "\n",
        "# Create the Scree Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(scree_data['PC'], scree_data['Variance'], marker='o', linestyle='-', color='black', markersize=3)  # Thin black line & points\n",
        "\n",
        "# Mark the elbow point\n",
        "if elbow_pc is not None:\n",
        "    plt.scatter(elbow_pc, scree_data['Variance'][elbow_pc - 1], s=30, c='black')  # Thin black elbow point\n",
        "\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Proportion of Variance')\n",
        "plt.title('Scree Plot (Elbow Method)')\n",
        "\n",
        "# Annotate the elbow point\n",
        "if elbow_pc is not None:\n",
        "    plt.annotate(f'Elbow at PC {elbow_pc}',\n",
        "                 (elbow_pc, scree_data['Variance'][elbow_pc - 1]),\n",
        "                 textcoords=\"offset points\",\n",
        "                 xytext=(0, 10),  # Adjust annotation position\n",
        "                 ha='center')\n",
        "\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SZwFWZwB9qNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Number of clusters can be adjusted\n",
        "\n",
        "# Fit KMeans to the scaled features and get cluster labels\n",
        "cluster_labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# Add the cluster labels to the original DataFrame, aligning with the correct index\n",
        "data['Cluster'] = -1  # Initialize with a placeholder\n",
        "data.loc[features.index, 'Cluster'] = cluster_labels  # Assign to the same rows used for clustering\n",
        "\n",
        "\n",
        "# Markov Chain Model: Estimating Transition Probabilities\n",
        "# We'll consider the sequence of cluster assignments as states in the Markov Chain\n",
        "\n",
        "# Create a transition matrix based on the cluster sequence\n",
        "transition_matrix = np.zeros((3, 3))  # Assuming 3 clusters\n",
        "\n",
        "# Iterate through the rows and calculate transitions\n",
        "for i in range(len(data) - 1):\n",
        "    current_state = data['Cluster'].iloc[i]\n",
        "    next_state = data['Cluster'].iloc[i + 1]\n",
        "\n",
        "    # Check if current or next state is -1 (placeholder) and skip\n",
        "    if current_state == -1 or next_state == -1:\n",
        "        continue\n",
        "\n",
        "    transition_matrix[current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrix by the row sums (so each row sums to 1)\n",
        "row_sums = transition_matrix.sum(axis=1)\n",
        "transition_matrix_normalized = transition_matrix / row_sums[:, np.newaxis]\n",
        "\n",
        "# Print the normalized transition matrix\n",
        "print(\"Transition Matrix (Normalized):\")\n",
        "print(transition_matrix_normalized)\n",
        "\n",
        "# Visualize the transition matrix as a heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(transition_matrix_normalized, cmap='Blues', interpolation='nearest')\n",
        "plt.colorbar(label='Transition Probability')\n",
        "plt.title('Markov Chain Transition Matrix')\n",
        "plt.xlabel('Next State')\n",
        "plt.ylabel('Current State')\n",
        "plt.xticks(np.arange(3), ['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
        "plt.yticks(np.arange(3), ['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNj5_B7z9Ic3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)  # Number of clusters can be adjusted\n",
        "\n",
        "# Fit KMeans to the scaled features and get cluster labels\n",
        "cluster_labels = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# Add the cluster labels to the original DataFrame, aligning with the correct index\n",
        "data['Cluster'] = -1  # Initialize with a placeholder\n",
        "data.loc[features.index, 'Cluster'] = cluster_labels  # Assign to the same rows used for clustering\n",
        "\n",
        "# Markov Chain Model: Estimating Transition Probabilities\n",
        "# We'll consider the sequence of cluster assignments as states in the Markov Chain\n",
        "\n",
        "# Create a transition matrix based on the cluster sequence\n",
        "transition_matrix = np.zeros((3, 3))  # Assuming 3 clusters\n",
        "\n",
        "# Iterate through the rows and calculate transitions\n",
        "for i in range(len(data) - 1):\n",
        "    current_state = data['Cluster'].iloc[i]\n",
        "    next_state = data['Cluster'].iloc[i + 1]\n",
        "\n",
        "    # Check if current or next state is -1 (placeholder) and skip\n",
        "    if current_state == -1 or next_state == -1:\n",
        "        continue\n",
        "\n",
        "    transition_matrix[current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrix by the row sums (so each row sums to 1)\n",
        "row_sums = transition_matrix.sum(axis=1)\n",
        "transition_matrix_normalized = transition_matrix / row_sums[:, np.newaxis]\n",
        "\n",
        "# Print the normalized transition matrix\n",
        "print(\"Transition Matrix (Normalized):\")\n",
        "print(transition_matrix_normalized)\n",
        "\n",
        "# Visualize the transition flow diagram using networkx\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes for each cluster\n",
        "for i in range(3):\n",
        "    G.add_node(f'Cluster {i}')\n",
        "\n",
        "# Add edges with weights (transition probabilities)\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        if transition_matrix_normalized[i, j] > 0:\n",
        "            G.add_edge(f'Cluster {i}', f'Cluster {j}', weight=transition_matrix_normalized[i, j])\n",
        "\n",
        "# Plot the directed graph (flow diagram)\n",
        "plt.figure(figsize=(8, 6))\n",
        "pos = nx.spring_layout(G, seed=42)  # Layout for the graph\n",
        "edges = G.edges(data=True)\n",
        "weights = [edata['weight'] for u, v, edata in edges]\n",
        "\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color='skyblue', font_size=12, font_weight='bold', arrowsize=20)\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{edata[\"weight\"]:.2f}' for u, v, edata in edges})\n",
        "\n",
        "plt.title('Markov Chain Transition Flow Diagram')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zqM1KE7B99aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/Clustering.csv', header=0)\n",
        "\n",
        "# Extract features\n",
        "features = data[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Discretize the features into discrete states (e.g., using quantiles or custom thresholds)\n",
        "n_states = 4  # Number of discrete states (bins)\n",
        "discrete_features = np.digitize(scaled_features, bins=np.linspace(-3, 3, n_states))\n",
        "\n",
        "# Initialize a transition matrix for each feature (6 features)\n",
        "transition_matrices = {feature: np.zeros((n_states, n_states)) for feature in features.columns}\n",
        "\n",
        "# Iterate through the rows and calculate transitions\n",
        "for i in range(len(features) - 1):\n",
        "    for j, feature in enumerate(features.columns):\n",
        "        current_state = discrete_features[i, j]\n",
        "        next_state = discrete_features[i + 1, j]\n",
        "        transition_matrices[feature][current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrices by the row sums (so each row sums to 1)\n",
        "for feature in features.columns:\n",
        "    row_sums = transition_matrices[feature].sum(axis=1)\n",
        "    transition_matrices[feature] /= row_sums[:, np.newaxis]\n",
        "\n",
        "# Print the normalized transition matrices for each feature\n",
        "for feature in features.columns:\n",
        "    print(f\"Transition Matrix for {feature}:\")\n",
        "    print(transition_matrices[feature])\n",
        "\n",
        "# Visualization of Transition Flow Diagrams for each feature\n",
        "for feature in features.columns:\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes for each state (discrete state)\n",
        "    for i in range(n_states):\n",
        "        G.add_node(f'State {i+1}')\n",
        "\n",
        "    # Add edges with weights (transition probabilities)\n",
        "    for i in range(n_states):\n",
        "        for j in range(n_states):\n",
        "            if transition_matrices[feature][i, j] > 0:\n",
        "                G.add_edge(f'State {i+1}', f'State {j+1}', weight=transition_matrices[feature][i, j])\n",
        "\n",
        "    # Plot the directed graph (flow diagram) for each feature\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(G, seed=42)  # Layout for the graph\n",
        "    edges = G.edges(data=True)\n",
        "    weights = [edata['weight'] for u, v, edata in edges]\n",
        "\n",
        "    nx.draw(G, pos, with_labels=True, node_size=3000, node_color='skyblue', font_size=12, font_weight='bold', arrowsize=20)\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{edata[\"weight\"]:.2f}' for u, v, edata in edges})\n",
        "\n",
        "    plt.title(f'Markov Chain Transition Flow Diagram for {feature}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HZ0k5SZv-JqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv(\"/content/Clustering.csv\", header=0)\n",
        "\n",
        "# Extract Features\n",
        "features = df[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Discretize the features into discrete states (e.g., using quantiles or custom thresholds)\n",
        "n_states = 4  # Number of discrete states (bins)\n",
        "discrete_features = np.digitize(scaled_features, bins=np.linspace(-3, 3, n_states))\n",
        "\n",
        "# Compute Co-Occurrence Matrix (Transition Counts)\n",
        "transition_matrix = np.zeros((n_states, n_states))  # Initialize the transition matrix for n_states\n",
        "\n",
        "# Iterate through the rows and calculate transitions\n",
        "for i in range(len(features) - 1):\n",
        "    for j in range(features.shape[1]):  # For each feature\n",
        "        current_state = discrete_features[i, j]\n",
        "        next_state = discrete_features[i + 1, j]\n",
        "        transition_matrix[current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrix (convert counts to probabilities)\n",
        "transition_df = pd.DataFrame(transition_matrix, index=[f\"State {i+1}\" for i in range(n_states)],\n",
        "                             columns=[f\"State {i+1}\" for i in range(n_states)])\n",
        "transition_df = transition_df.div(transition_df.sum(axis=1), axis=0).fillna(0)\n",
        "\n",
        "# Create Markov Chain Graph\n",
        "G = nx.DiGraph()\n",
        "for i in transition_df.index:\n",
        "    for j in transition_df.columns:\n",
        "        if transition_df.loc[i, j] > 0:  # Only add transitions with probability > 0\n",
        "            G.add_edge(i, j, weight=transition_df.loc[i, j])\n",
        "\n",
        "# Generate Pastel Colors for Nodes (States)\n",
        "colors = cm.Pastel1(np.linspace(0, 1, len(transition_df.columns)))\n",
        "\n",
        "# Assign colors to nodes in the graph\n",
        "node_color_map = {state: color for state, color in zip(transition_df.columns, colors)}\n",
        "\n",
        "# Draw Graph with Pastel Colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "edges = G.edges(data=True)\n",
        "edge_weights = [d['weight'] * 5 for (u, v, d) in edges]  # Scale edge width\n",
        "\n",
        "# Draw nodes with pastel colors\n",
        "node_colors = [node_color_map[node] for node in G.nodes]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\", width=edge_weights, node_size=2000)\n",
        "\n",
        "plt.title(\"Markov Chain Model: Feature Transitions (Pastel Colors)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NO6U2pf-A7wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv(\"/content/Clustering.csv\", header=0)\n",
        "\n",
        "# Extract Features\n",
        "features = df[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Discretize the features into discrete states (e.g., using quantiles or custom thresholds)\n",
        "n_states = 4  # Number of discrete states (bins)\n",
        "discrete_features = np.digitize(scaled_features, bins=np.linspace(-3, 3, n_states))\n",
        "\n",
        "# Compute Co-Occurrence Matrix (Transition Counts)\n",
        "# Each column corresponds to a specific feature, and each row to a discretized state\n",
        "transition_matrix = np.zeros((features.shape[1], n_states, n_states))  # For each feature\n",
        "\n",
        "# Iterate through the rows and calculate transitions for each feature\n",
        "for i in range(len(features) - 1):\n",
        "    for j in range(features.shape[1]):  # For each feature\n",
        "        current_state = discrete_features[i, j]\n",
        "        next_state = discrete_features[i + 1, j]\n",
        "        transition_matrix[j, current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrix (convert counts to probabilities)\n",
        "transition_df = pd.DataFrame()\n",
        "for feature_idx in range(features.shape[1]):\n",
        "    matrix = transition_matrix[feature_idx]\n",
        "    matrix = matrix / matrix.sum(axis=1, keepdims=True)  # Normalize each row\n",
        "    transition_df[features.columns[feature_idx]] = matrix.flatten()\n",
        "\n",
        "# Create Markov Chain Graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Create edges based on transitions for each feature\n",
        "for feature_idx in range(features.shape[1]):\n",
        "    feature_name = features.columns[feature_idx]\n",
        "    for i in range(n_states):\n",
        "        for j in range(n_states):\n",
        "            weight = transition_matrix[feature_idx, i, j]\n",
        "            if weight > 0:  # Only add transitions with non-zero probability\n",
        "                G.add_edge(f\"{feature_name} - State {i+1}\", f\"{feature_name} - State {j+1}\", weight=weight)\n",
        "\n",
        "# Generate Pastel Colors for Nodes (States)\n",
        "colors = cm.Pastel1(np.linspace(0, 1, len(G.nodes)))\n",
        "\n",
        "# Assign colors to nodes in the graph\n",
        "node_color_map = {node: color for node, color in zip(G.nodes, colors)}\n",
        "\n",
        "# Draw Graph with Pastel Colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "edges = G.edges(data=True)\n",
        "edge_weights = [d['weight'] * 5 for (u, v, d) in edges]  # Scale edge width\n",
        "\n",
        "# Draw nodes with pastel colors\n",
        "node_colors = [node_color_map[node] for node in G.nodes]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\", width=edge_weights, node_size=2000)\n",
        "\n",
        "plt.title(\"Markov Chain Model: Feature Transitions (Pastel Colors with States)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "zSae-rZyBLdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv(\"/content/Clustering.csv\", header=0)\n",
        "\n",
        "# Extract Features\n",
        "features = df[['Lipase', 'Protease', 'Hemolysin', 'DNase', 'Staphyloxanthin', 'EsxA']]\n",
        "\n",
        "# Remove rows with any missing values\n",
        "features = features.dropna()\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# Discretize the features into discrete states (e.g., using quantiles or custom thresholds)\n",
        "n_states = 4  # Number of discrete states (bins)\n",
        "discrete_features = np.digitize(scaled_features, bins=np.linspace(-3, 3, n_states))\n",
        "\n",
        "# Compute Co-Occurrence Matrix (Transition Counts)\n",
        "transition_matrix = np.zeros((features.shape[1], n_states, n_states))  # For each feature\n",
        "\n",
        "# Iterate through the rows and calculate transitions for each feature\n",
        "for i in range(len(features) - 1):\n",
        "    for j in range(features.shape[1]):  # For each feature\n",
        "        current_state = discrete_features[i, j]\n",
        "        next_state = discrete_features[i + 1, j]\n",
        "        transition_matrix[j, current_state, next_state] += 1\n",
        "\n",
        "# Normalize the transition matrix (convert counts to probabilities)\n",
        "transition_df = pd.DataFrame()\n",
        "for feature_idx in range(features.shape[1]):\n",
        "    matrix = transition_matrix[feature_idx]\n",
        "    matrix = matrix / matrix.sum(axis=1, keepdims=True)  # Normalize each row\n",
        "    transition_df[features.columns[feature_idx]] = matrix.flatten()\n",
        "\n",
        "# Create Markov Chain Graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Create edges based on transitions for each feature\n",
        "for feature_idx in range(features.shape[1]):\n",
        "    feature_name = features.columns[feature_idx]\n",
        "    for i in range(n_states):\n",
        "        for j in range(n_states):\n",
        "            weight = transition_matrix[feature_idx, i, j]\n",
        "            if weight > 0:  # Only add transitions with non-zero probability\n",
        "                G.add_edge(f\"{feature_name} - State {i+1}\", f\"{feature_name} - State {j+1}\", weight=weight)\n",
        "\n",
        "# Generate Pastel Colors for Nodes (States)\n",
        "colors = cm.Pastel1(np.linspace(0, 1, len(G.nodes)))\n",
        "\n",
        "# Assign colors to nodes in the graph\n",
        "node_color_map = {node: color for node, color in zip(G.nodes, colors)}\n",
        "\n",
        "# Draw Graph with Pastel Colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "edges = G.edges(data=True)\n",
        "edge_weights = [d['weight'] * 5 for (u, v, d) in edges]  # Scale edge width\n",
        "\n",
        "# Draw nodes with pastel colors\n",
        "node_colors = [node_color_map[node] for node in G.nodes]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\", width=edge_weights, node_size=2000)\n",
        "\n",
        "plt.title(\"Markov Chain Model: Feature Transitions (Pastel Colors with States)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z2UWemVvBb46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# Load your Clustering Data (Replace with correct file path if necessary)\n",
        "df = pd.read_csv(\"/content/Clustering.csv\", header=0)\n",
        "\n",
        "# Replace resistance values if applicable (or adjust for your features)\n",
        "# Example: if features are categorical, you can binarize them or treat as continuous\n",
        "df_binary = df.applymap(lambda x: 1 if x > 0 else 0)  # Adjust this for your dataset logic\n",
        "\n",
        "# Compute Co-Occurrence Matrix (Transition Counts)\n",
        "# Co-occurrence is calculated by dot product of binary DataFrame (rows x columns)\n",
        "transition_matrix = np.dot(df_binary.T, df_binary)  # Co-occurrence matrix\n",
        "transition_df = pd.DataFrame(transition_matrix, index=df.columns, columns=df.columns)\n",
        "\n",
        "# Normalize Each Row (Convert Counts to Probabilities)\n",
        "transition_df = transition_df.div(transition_df.sum(axis=1), axis=0).fillna(0)  # Normalize the rows\n",
        "\n",
        "# Create Markov Chain Graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph based on the co-occurrence matrix with weights\n",
        "for i in transition_df.index:\n",
        "    for j in transition_df.columns:\n",
        "        if transition_df.loc[i, j] > 0:  # Only add transitions with probability > 0\n",
        "            G.add_edge(i, j, weight=transition_df.loc[i, j])\n",
        "\n",
        "# Generate Pastel Colors for Nodes (Features)\n",
        "colors = cm.Pastel1(np.linspace(0, 1, len(df.columns)))\n",
        "\n",
        "# Assign colors to nodes in the graph (feature names)\n",
        "node_color_map = {feature: color for feature, color in zip(df.columns, colors)}\n",
        "\n",
        "# Draw Graph with Pastel Colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)  # Positioning nodes using spring layout\n",
        "edges = G.edges(data=True)\n",
        "edge_weights = [d['weight'] * 5 for (u, v, d) in edges]  # Scale edge width for better visualization\n",
        "\n",
        "# Draw nodes with pastel colors\n",
        "node_colors = [node_color_map[node] for node in G.nodes]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\", width=edge_weights, node_size=2000)\n",
        "\n",
        "# Add title to the plot\n",
        "plt.title(\"Markov Chain Model: Feature Co-occurrence Transition (Pastel Colors)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-sqH13qeCcS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.cm as cm\n",
        "from numpy.linalg import eig\n",
        "\n",
        "# Load your Clustering Data (Replace with correct file path if necessary)\n",
        "df = pd.read_csv(\"/content/Clustering.csv\", header=0)\n",
        "\n",
        "# Replace resistance values if applicable (or adjust for your features)\n",
        "df_binary = df.applymap(lambda x: 1 if x > 0 else 0)  # Adjust this for your dataset logic\n",
        "\n",
        "# Compute Co-Occurrence Matrix (Transition Counts)\n",
        "transition_matrix = np.dot(df_binary.T, df_binary)  # Co-occurrence matrix\n",
        "transition_df = pd.DataFrame(transition_matrix, index=df.columns, columns=df.columns)\n",
        "\n",
        "# Normalize Each Row (Convert Counts to Probabilities)\n",
        "transition_df = transition_df.div(transition_df.sum(axis=1), axis=0).fillna(0)  # Normalize the rows\n",
        "\n",
        "# Create Markov Chain Graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph based on the co-occurrence matrix with weights\n",
        "for i in transition_df.index:\n",
        "    for j in transition_df.columns:\n",
        "        if transition_df.loc[i, j] > 0:  # Only add transitions with probability > 0\n",
        "            G.add_edge(i, j, weight=transition_df.loc[i, j])\n",
        "\n",
        "# Generate Pastel Colors for Nodes (Features)\n",
        "colors = cm.Pastel1(np.linspace(0, 1, len(df.columns)))\n",
        "\n",
        "# Assign colors to nodes in the graph (feature names)\n",
        "node_color_map = {feature: color for feature, color in zip(df.columns, colors)}\n",
        "\n",
        "# Draw Graph with Pastel Colors\n",
        "plt.figure(figsize=(12, 8))\n",
        "pos = nx.spring_layout(G, seed=42)  # Positioning nodes using spring layout\n",
        "edges = G.edges(data=True)\n",
        "edge_weights = [d['weight'] * 5 for (u, v, d) in edges]  # Scale edge width for better visualization\n",
        "\n",
        "# Draw nodes with pastel colors\n",
        "node_colors = [node_color_map[node] for node in G.nodes]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\", width=edge_weights, node_size=2000)\n",
        "\n",
        "# Add title to the plot\n",
        "plt.title(\"Markov Chain Model: Feature Co-occurrence Transition (Pastel Colors)\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate Stationary Distribution (Eigenvector for eigenvalue 1)\n",
        "transition_matrix_normalized = transition_df.to_numpy()\n",
        "eigvals, eigvecs = eig(transition_matrix_normalized.T)  # Transpose for right eigenvectors\n",
        "\n",
        "# Find the eigenvector corresponding to eigenvalue 1 (stationary distribution)\n",
        "stationary_vector = eigvecs[:, np.isclose(eigvals, 1)].real.flatten()\n",
        "\n",
        "# Normalize the stationary vector (it should sum to 1)\n",
        "stationary_distribution = stationary_vector / stationary_vector.sum()\n",
        "\n",
        "# Print the stationary distribution (long-term probabilities)\n",
        "print(\"\\nStationary Distribution (Long-Term Probabilities):\")\n",
        "for feature, prob in zip(df.columns, stationary_distribution):\n",
        "    print(f\"{feature}: {prob:.4f}\")\n",
        "\n",
        "# First-Passage Time Calculation (Expected Number of Steps to Reach a State)\n",
        "# This involves computing the inverse of the transition matrix, which can be complex.\n",
        "# Here, we will compute the mean first-passage times for each pair of states using a simpler method.\n",
        "first_passage_matrix = np.linalg.inv(np.eye(transition_matrix_normalized.shape[0]) - transition_matrix_normalized)\n",
        "first_passage_df = pd.DataFrame(first_passage_matrix, index=df.columns, columns=df.columns)\n",
        "\n",
        "# Visualize the First-Passage Times Matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(first_passage_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'First-Passage Time'})\n",
        "plt.title(\"First-Passage Times Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "OThYLmZ3C0Au"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}